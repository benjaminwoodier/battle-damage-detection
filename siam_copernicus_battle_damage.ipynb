{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Install all necessary dependencies - Copernicus requires TorchGeo, and the below Torch and Torchvision Instances\n",
        "!pip install opencv-python-headless==4.10.0.82\n",
        "!pip install thinc==8.2.2\n",
        "!pip install tensorflow==2.18.0\n",
        "!pip install cupy-cuda11x\n",
        "!pip install torch==2.1.0 torchvision==0.16.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install torchgeo geopandas rasterio scikit-learn matplotlib pandas shapely"
      ],
      "metadata": {
        "id": "Va6zhWehPwa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Numpy 1.26.4\n",
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "Au2l4usvPa78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, jaccard_score\n",
        "import matplotlib.pyplot as plt\n",
        "from torchgeo.models import CopernicusFM\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import torchvision.transforms as T\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "Yb_Ovfe7PcwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define dataset (Google Drive Method), generate or download metadata where necessary, select augmented, extended, or standard dataset.\n",
        "USE_AUGMENTED_DATA = True\n",
        "if USE_AUGMENTED_DATA:\n",
        "    original_metadata = \"/content/drive/MyDrive/SGDS_BD_DISS_DATA/BD_copernicus_metadata.csv\"\n",
        "    augmented_metadata = \"/content/drive/MyDrive/SGDS_BD_DISS_DATA/AUG_copernicus_metadata.csv\"\n",
        "    if os.path.exists(augmented_metadata):\n",
        "        df_main = pd.read_csv(original_metadata)\n",
        "        df_aug = pd.read_csv(augmented_metadata)\n",
        "        df_combined = pd.concat([df_main, df_aug], ignore_index=True)\n",
        "        combined_metadata_path = \"/content/drive/MyDrive/SGDS_BD_DISS_DATA/COMBINED_METADATA.csv\"\n",
        "        df_combined.to_csv(combined_metadata_path, index=False)\n",
        "        metadata_csv = combined_metadata_path\n",
        "    else:\n",
        "        metadata_csv = original_metadata\n",
        "else:\n",
        "    metadata_csv = \"/content/drive/MyDrive/SGDS_BD_DISS_DATA/BD_copernicus_metadata.csv\"\n"
      ],
      "metadata": {
        "id": "GDicK9DrZlcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Class, designed to function with CopFM encoder.\n",
        "class CopernicusDataset(Dataset):\n",
        "    def __init__(self, metadata_csv):\n",
        "        self.data = pd.read_csv(metadata_csv)\n",
        "        self.tiles = self.data['path'].apply(lambda p: os.path.basename(p).split('_')[2])\n",
        "        self.tile_ids = sorted(set(self.tiles))\n",
        "        self.resize = T.Resize((256, 256), antialias=True)\n",
        "\n",
        "        valid_ids = []\n",
        "        for tile_id in self.tile_ids:\n",
        "            mask_path = self._get_mask_path(tile_id)\n",
        "            if mask_path and os.path.exists(mask_path):\n",
        "                with rasterio.open(mask_path) as src:\n",
        "                    mask = src.read(1)\n",
        "                if np.any(mask > 0):\n",
        "                    valid_ids.append(tile_id)\n",
        "        self.tile_ids = valid_ids\n",
        "        self.tile_ids.sort(key=lambda tid: -self._damage_ratio(tid))\n",
        "\n",
        "    #Acquire ground truth masks\n",
        "    def _get_mask_path(self, tile_id):\n",
        "        match = self.data[self.data['path'].str.contains(f\"tile_{tile_id}_S1_PostMean\")]\n",
        "        if match.empty:\n",
        "            return None\n",
        "        row = match.iloc[0]\n",
        "        region = row['region'].upper()\n",
        "        return os.path.join('/content/drive/MyDrive/SGDS_BD_DISS_DATA', f\"{region}_MASKS\", f\"tile_{tile_id}_mask.tif\")\n",
        "\n",
        "    #Damage ratio function\n",
        "    def _damage_ratio(self, tile_id):\n",
        "        mask_path = self._get_mask_path(tile_id)\n",
        "        if not mask_path or not os.path.exists(mask_path):\n",
        "            return 0.0\n",
        "        with rasterio.open(mask_path) as src:\n",
        "            mask = src.read(1)\n",
        "        return np.mean(mask == 2)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tile_ids)\n",
        "\n",
        "    #Image resizing (upscaling) to 256x256 for 50x50px tiles to work with CopFM pretrain weights.\n",
        "    def __getitem__(self, idx):\n",
        "        tile_id = self.tile_ids[idx]\n",
        "        tile_rows = self.data[self.data['path'].str.contains(f\"tile_{tile_id}_\")]\n",
        "        pre_row = tile_rows[tile_rows['path'].str.contains('PreMean')].iloc[0]\n",
        "        post_row = tile_rows[tile_rows['path'].str.contains('PostMean')].iloc[0]\n",
        "\n",
        "        with rasterio.open(pre_row['path']) as src:\n",
        "            pre_img = src.read(out_shape=(2, 50, 50))\n",
        "        with rasterio.open(post_row['path']) as src:\n",
        "            post_img = src.read(out_shape=(2, 50, 50))\n",
        "\n",
        "        img = np.concatenate([pre_img, post_img], axis=0)\n",
        "        img = torch.tensor(img, dtype=torch.float32) / 255.0\n",
        "        img = T.Resize((256, 256))(img)\n",
        "\n",
        "        mask_path = self._get_mask_path(tile_id)\n",
        "        with rasterio.open(mask_path) as src:\n",
        "            mask = src.read(1, out_shape=(50, 50))\n",
        "        mask = torch.tensor(mask, dtype=torch.long)\n",
        "        mask = T.Resize((256, 256))(mask.unsqueeze(0)).squeeze(0).long()\n",
        "\n",
        "        delta_time = 20000.0\n",
        "        patch_area = 1.0\n",
        "        metadata = torch.tensor([post_row['lon'], post_row['lat'], delta_time, patch_area], dtype=torch.float32)\n",
        "\n",
        "        return img, mask, metadata"
      ],
      "metadata": {
        "id": "JJ-VWYO4QOlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#U-Net Style Decoder (in line with Siamese U-Net Model)\n",
        "class UNetDecoder(nn.Module):\n",
        "    def __init__(self, in_channels=1024, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.up1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, 512, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.up2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.up3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.final = nn.Conv2d(128, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up1(x)\n",
        "        x = self.up2(x)\n",
        "        x = self.up3(x)\n",
        "        return self.final(x)"
      ],
      "metadata": {
        "id": "FMrkWjk_QZHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Siamese Segmentation Trainer, finding the difference in pre and post tokens to optimally learn change between SAR images.\n",
        "class SiameseSegmentationTrainer(nn.Module):\n",
        "    def __init__(self, weight_path, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.encoder_pre = CopernicusFM(\n",
        "            img_size=256,\n",
        "            patch_size=16,\n",
        "            num_classes=0,\n",
        "            global_pool=False,\n",
        "            embed_dim=1024,\n",
        "            depth=24,\n",
        "            num_heads=16\n",
        "        )\n",
        "        self.encoder_post = CopernicusFM(\n",
        "            img_size=256,\n",
        "            patch_size=16,\n",
        "            num_classes=0,\n",
        "            global_pool=False,\n",
        "            embed_dim=1024,\n",
        "            depth=24,\n",
        "            num_heads=16\n",
        "        )\n",
        "\n",
        "        weights = torch.load(weight_path, map_location='cpu')\n",
        "        if \"pos_embed\" in weights:\n",
        "            old_pos_embed = weights[\"pos_embed\"]\n",
        "            cls_token = old_pos_embed[:, :1, :]\n",
        "            old_grid = int((old_pos_embed.shape[1] - 1) ** 0.5)\n",
        "            new_grid = 16  # 256 / 16\n",
        "            num_patches = new_grid * new_grid\n",
        "\n",
        "            old_pe = old_pos_embed[:, 1:, :].reshape(1, old_grid, old_grid, -1).permute(0, 3, 1, 2)\n",
        "            new_pe = F.interpolate(old_pe, size=(new_grid, new_grid), mode='bilinear', align_corners=False)\n",
        "            new_pe = new_pe.permute(0, 2, 3, 1).reshape(1, num_patches, -1)\n",
        "            weights[\"pos_embed\"] = torch.cat([cls_token, new_pe], dim=1)\n",
        "\n",
        "        self.encoder_pre.load_state_dict(weights, strict=False)\n",
        "        self.encoder_post.load_state_dict(weights, strict=False)\n",
        "\n",
        "        self.decoder = UNetDecoder(in_channels=1024, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x, meta):\n",
        "        pre = x[:, :2]\n",
        "        post = x[:, 2:]\n",
        "        wavelengths = torch.tensor([56000000.0] * 2, device=x.device)\n",
        "        bandwidths = torch.tensor([100000000.0] * 2, device=x.device)\n",
        "\n",
        "        pre_tokens = self.encoder_pre.patch_embed_spectral(pre, wavelengths, bandwidths, kernel_size=7)\n",
        "        post_tokens = self.encoder_post.patch_embed_spectral(post, wavelengths, bandwidths, kernel_size=7)\n",
        "\n",
        "        for blk in self.encoder_pre.blocks:\n",
        "            pre_tokens = blk(pre_tokens)\n",
        "        for blk in self.encoder_post.blocks:\n",
        "            post_tokens = blk(post_tokens)\n",
        "\n",
        "        diff_tokens = post_tokens - pre_tokens\n",
        "\n",
        "        B, N, C = diff_tokens.shape\n",
        "        if N == 257:\n",
        "            diff_tokens = diff_tokens[:, 1:, :]\n",
        "            N -= 1\n",
        "\n",
        "        H = W = int(N ** 0.5)\n",
        "        feats = diff_tokens.permute(0, 2, 1).reshape(B, C, H, W)\n",
        "        out = self.decoder(feats)\n",
        "        return F.interpolate(out, size=(50, 50), mode='bilinear', align_corners=False)"
      ],
      "metadata": {
        "id": "RR7nZFkFQat-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Composite loss function combining CE, Dice and Focal loss (final version)\n",
        "def dice_loss(pred, target, epsilon=1e-6):\n",
        "    pred = F.softmax(pred, dim=1)\n",
        "    target_onehot = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2).float()\n",
        "    intersection = (pred * target_onehot).sum(dim=(2, 3))\n",
        "    union = pred.sum(dim=(2, 3)) + target_onehot.sum(dim=(2, 3))\n",
        "    dice = (2 * intersection + epsilon) / (union + epsilon)\n",
        "    return 1 - dice.mean()\n",
        "\n",
        "def focal_loss(pred, target, alpha=1.0, gamma=2.0):\n",
        "    ce_loss = F.cross_entropy(pred, target, reduction='none')\n",
        "    pt = torch.exp(-ce_loss)\n",
        "    focal = alpha * (1 - pt) ** gamma * ce_loss\n",
        "    return focal.mean()\n",
        "\n",
        "def total_loss(pred, target):\n",
        "    weights = torch.tensor([0.5, 1.0, 5.0], device=pred.device)\n",
        "    return (\n",
        "        0.5 * F.cross_entropy(pred, target, weight=weights) +\n",
        "        0.3 * dice_loss(pred, target) +\n",
        "        0.2 * focal_loss(pred, target)\n",
        "    )"
      ],
      "metadata": {
        "id": "UpnG51FURAVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation\n",
        "def evaluate(model, dataloader, device, num_classes=3):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y, meta in dataloader:\n",
        "            x, y, meta = x.to(device), y.to(device), meta.to(device)\n",
        "            y = F.interpolate(y.unsqueeze(1).float(), size=(50, 50), mode='nearest').squeeze(1).long()\n",
        "            logits = model(x, meta)\n",
        "            preds = logits.argmax(1)\n",
        "            all_preds.append(preds.cpu().numpy().flatten())\n",
        "            all_labels.append(y.cpu().numpy().flatten())\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    report = classification_report(\n",
        "        all_labels, all_preds, labels=list(range(num_classes)),\n",
        "        target_names=[\"background\", \"intact\", \"damaged\"],\n",
        "        digits=3, output_dict=True\n",
        "    )\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    macro_iou = jaccard_score(all_labels, all_preds, average='macro')\n",
        "    acc = (all_preds == all_labels).mean()\n",
        "    return acc, macro_f1, macro_iou, report\n"
      ],
      "metadata": {
        "id": "B5skJVzRRDgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import torch.nn.functional as F\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#resume_from = os.path.join(save_dir, \"\")  #Path to resume checkpoint if necessary\n",
        "\n",
        "#Load Copernicus Pretrain\n",
        "model = SiameseSegmentationTrainer(\"/content/drive/MyDrive/copernicus_fm/CopernicusFM_ViT_large_varlang_e100.pth\").to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scaler = GradScaler()\n",
        "best_f1 = 0.0\n",
        "\n",
        "#Path to resume checkpoint\n",
        "#if os.path.exists(resume_from):\n",
        "    #print(f\"Resuming from checkpoint: {resume_from}\")\n",
        "    #checkpoint = torch.load(resume_from)\n",
        "    #model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    #start_epoch = checkpoint.get('epoch', 0)\n",
        "    #best_f1 = checkpoint.get('val_f1', 0.0)\n",
        "\n",
        "dataset = CopernicusDataset(metadata_csv)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=2)\n",
        "\n",
        "#Training Loop\n",
        "for epoch in range(start_epoch, 50):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for x, y, meta in train_loader:\n",
        "        x, meta = x.to(device), meta.to(device)\n",
        "        y = F.interpolate(y.unsqueeze(1).float(), size=(50, 50), mode='nearest').squeeze(1).long()\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            logits = model(x, meta)\n",
        "            loss = total_loss(logits, y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "    acc, f1, iou, report = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f} | Val Acc: {acc:.4f} | Macro F1: {f1:.4f} | Macro IoU: {iou:.4f}\")\n",
        "    for cls in [\"background\", \"intact\", \"damaged\"]:\n",
        "        f1_cls = report[cls][\"f1-score\"]\n",
        "        iou_cls = report[cls][\"recall\"] * report[cls][\"precision\"] / (report[cls][\"recall\"] + report[cls][\"precision\"] + 1e-6)\n",
        "        print(f\"  {cls.capitalize():<10} F1: {f1_cls:.3f}, IoU (est): {iou_cls:.3f}\")\n",
        "\n",
        "    #Save best model\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': acc,\n",
        "            'val_f1': f1,\n",
        "            'val_iou': iou\n",
        "        }, os.path.join(save_dir, \"copernicus_aug_ext_siam_best_model.pth\"))\n",
        "        print(f\"Saved best model (Epoch {epoch+1}, F1={f1:.4f})\")\n",
        "\n",
        "    #Save latest model (every epoch)\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'val_acc': acc,\n",
        "        'val_f1': f1,\n",
        "        'val_iou': iou\n",
        "    }, os.path.join(save_dir, \"copernicus_au_ext_siam_model.pth\"))\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "XnnbHVYuRJcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation and Visualisation\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#checkpoint = torch.load(\"/content/drive/MyDrive/copernicus_aug_siam_model.pth\")\n",
        "#model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    #Load sample\n",
        "    x, y, meta = dataset[0]\n",
        "\n",
        "    #Recover region from dataset for visualisation\n",
        "    tile_id = dataset.tile_ids[0]\n",
        "    row = dataset.data[dataset.data['path'].str.contains(f\"tile_{tile_id}_\")].iloc[0]\n",
        "    region = row['region']\n",
        "\n",
        "    x = x.unsqueeze(0).to(device)\n",
        "    meta = meta.unsqueeze(0).to(device)\n",
        "\n",
        "    #Run pred\n",
        "    pred = model(x, meta)\n",
        "    pred = pred.argmax(1)\n",
        "\n",
        "    pred_mask = F.interpolate(pred.unsqueeze(1).float(), size=(50, 50), mode='nearest').squeeze().cpu().int().numpy()\n",
        "\n",
        "    x_np = x.squeeze().cpu().numpy()\n",
        "    vv_pre = x_np[0]\n",
        "    vv_post = x_np[2]\n",
        "\n",
        "    #Plot vis\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    axs[0].imshow(vv_pre, cmap='gray'); axs[0].set_title(f\"{region.capitalize()} VV Pre-War\")\n",
        "    axs[1].imshow(vv_post, cmap='gray'); axs[1].set_title(f\"{region.capitalize()} VV Post-War\")\n",
        "    axs[2].imshow(y.cpu(), cmap='viridis', vmin=0, vmax=2); axs[2].set_title(f\"{region.capitalize()} Ground Truth\")\n",
        "    axs[3].imshow(pred_mask, cmap='viridis', vmin=0, vmax=2); axs[3].set_title(f\"{region.capitalize()} Predicted\")\n",
        "\n",
        "    for ax in axs: ax.axis('off')\n",
        "    plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "tvVVZivvRdFa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}